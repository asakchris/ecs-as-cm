# AWS ECS Auto-Scaling using Custom Metrics
- ECS measures service utilization based on CPU and memory resources consumed by the tasks 
  that belong to a service and publishes CloudWatch metrics (ECSServiceAverageCPUUtilization 
  and ECSServiceAverageMemoryUtilization). Application Auto Scaling can then use these 
  predefined metrics in conjunction with scaling policies to proportionally scale the number 
  of tasks in a service.
- There are several use cases where a serviceâ€™s average CPU and memory usage alone are not 
  reliable indicators of when and to what degree to execute a scaling action. Custom metrics
  like no of HTTP requests received, and no of pending messages from a queue, be better suited 
  to trigger scaling actions.
- ECS also supports scaling a service based on a custom metric specification that represents 
  a CloudWatch metric. These metrics can be a custom one published from the application or it
  can be the one generated by other AWS services like Amazon MQ, and SQS etc.
- AWS application auto-scaling can be performed using one of the following: 
  `Target Tracking (Policy)`/`Step Scaling (Policy)`/`Scheduled scaling (Action)`. Target tracking
  scaling policy is used in this example.
  - `Target Tracking Scaling Policy`
    - It increases or decreases the number of tasks that the service runs based on a target value 
      for a specific CloudWatch metric (predefined or customized).
    - It assumes that it should scale-out tasks when the specified metric is above the 
      target value and scale-in when it is below the target value.
    - ECS creates and manages the CloudWatch alarms that trigger the scaling policy and calculates 
      the scaling adjustment based on the CloudWatch metric and the target value specified. AWS
      controls the scaling adjustments automatically based on the target value specified.

## Services
There are 3 services:
- Producer
  - It exposes endpoints to send messages to `test.one` & `test.two` queues.
- Consumer
  - It consumes messages in `test.one` queue.
  - There are 5 listeners per instance.
  - Average processing time per message is 1 minute.
- Consumer2
  - It consumes messages in `test.two` queue.
  - There are 5 listeners per instance.
  - Average processing time per message is 1 minute.

## Architecture
![architecture](images/ecs-auto-scaling.png)

## Auto-Scaling
### Using metrics generated by Amazon MQ ActiveMQ
- Amazon MQ ActiveMQ service creates a metric called `QueueSize` which represents no of pending
  messages in a queue. It also includes the messages which are under processing by the
  consumers.
- Prefetch size in consumers needs to be set as 0 so that it will not fetch more than 1 message
  at a time. Refer [this](./consumer/src/main/resources/application.yml) file.
- Concurrency is set as 5, so it runs 5 costumers for queue `test.one`. 
  Refer [this](./consumer/src/main/resources/application.yml) file.
- Refer [this](./deployment/cfn/consumer-ecs-service.yml) file for auto-scaling configuration.
- It creates 2 alarms:
  - `Alarm High`
    - Threshold - QueueSize > 10 for 3 datapoints within 3 minutes
    - When in alarm, it executes scale-out action.
  - `Alarm Low`
    - Threshold - QueueSize < 9 for 15 datapoints within 15 minutes
    - When in alarm, it executes scale-in action.

### Using computed metrics on the fly
- Prefetch size in consumers needs to be set as 0 so that it will not fetch more than 1 message
  at a time. Refer [this](./consumer2/src/main/resources/application.yml) file.
- Concurrency is set as 5, so it runs 5 costumers for queue `test.two`.
  Refer [this](./consumer2/src/main/resources/application.yml) file.
- ECS supports scaling policies that are based on custom Amazon CloudWatch metrics, 
  which are evaluated against a metric math expression.
- Following are the metrics used to in auto-scaling: 
  - `Acceptable backlog per instance`
    - It is the target value used in scaling policy.
    - It is the acceptable latency value divided by the average time that an ECS task takes 
      to process a message and multiply it with no of listeners per instance.
  - `Backlog per instance`
    - Metric math expression is used to create this custom metric.
    - It is obtained by dividing the number of messages yet to be consumed within the queue
      with the total number of tasks running at the moment.
- Example calculation:
  - Processing time per message = 1 min
  - Acceptable latency = 3 min
  - No of listeners per instance = 5
  - Acceptable backlog per instance = 3 / 1 * 5 = 15 messages
  - No of instances = 2
  - No of pending messages = 100
  - Backlog per instance = 100 / 2 = 50 messages
- Refer [this](./deployment/cfn/env/DEV/Consumer2ScalingPolicy.json) file for auto-scaling 
  configuration.
- It creates 2 alarms:
  - `Alarm High`
    - Threshold - msg_backlog_per_instance_avg_1m > 15 for 3 datapoints within 3 minutes
    - When in alarm, it executes scale-out action.
  - `Alarm Low`
    - Threshold - msg_backlog_per_instance_avg_1m < 13.5 for 15 datapoints within 15 minutes
    - When in alarm, it executes scale-in action.

### Build
Run [this](.run/Build.run.xml) one from IDE to build the application and push docker images to remote repository.

### Run
###### Local
1. Download & install ActiveMQ from [here](https://activemq.apache.org/components/classic/download/).
   After installation access web console using [this](http://localhost:8161/) link.
2. Run [this](.run/Producer.run.xml) one from IDE to start `Producer` service.
3. Run [this](.run/Consumer.run.xml) one from IDE to start `Consumer` service.
4. Run [this](.run/Consumer2.run.xml) one from IDE to start `Consumer2` service.

###### docker compose
1. Run `docker-compose up -d` from root of the repository folder to start all 3 services along with Active MQ.
2. Run `docker-compose ps` to check status of services.
3. Run `docker-compose down` to stop all services.
4. Use below commands to check logs:
```
docker-compose logs -f --tail="all"
docker-compose logs -f --tail="100"

docker-compose logs -f --tail="all" producer
docker-compose logs -f --tail="all" consumer
```
###### AWS
Refer [this](./deployment/README.md).

### Test
1. Use below command to submit 10 messages into `test.one` queue:
   `curl --location 'http://localhost:18611/send-messages-1'`
2. Use below command to submit 10 messages into `test.two` queue:
   `curl --location 'http://localhost:18611/send-messages-2'`
